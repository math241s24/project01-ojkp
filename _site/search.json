[
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Project Proposal",
    "section": "",
    "text": "Proposal\nYour proposal should include:\n\nA brief description of your dataset including its provenance, dimensions, etc. (Make sure to load the data and use inline code for some of this information.)\nThe reason why you chose this dataset.\nThe two questions you want to answer.\n\nA plan for answering each of the questions including the variables involved, variables to be created (if any), external data to be merged in (if any).\n\n\n\nWe are using air quality data from the EPA (Environmental Protection Agency). Their data records atmospheric concentrations of major pollutants (CO, NO2, O3, PM 2.5, and SO2) every day of the year in geographic areas across the United States. The data can be as granular as county level; for our research purposes, we are focusing on the Portland metropolitan area (Portland-Hillsboro-Vancouver) and using data from 2017-2023. For our two research questions, we have subsetted the EPA data into two forms for each pollutant type: a 84 x 2 monthly average air pollutant and a 28606 x 4 daily air pollutant data. The specific number of observations differs by data set, due to the dataset having information from multiple sites in the Portland area. We have 10 datasets (5 of each type). Below is a glimpse into how our two forms of datasets are structured for one of our chosen top pollutants (PM2.5) which is 2 out of our 10 datasets.\n\nDaily data about PM2.5 mean concentration from each site in Portland-Vancouver-Hillsboro from 2017 to 2023:\n\n\nRows: 28,606\nColumns: 4\n$ Date                             &lt;date&gt; 2017-06-16, 2017-06-17, 2017-06-18, …\n$ `Daily Mean PM2.5 Concentration` &lt;dbl&gt; 2.3, 3.7, 3.0, 5.4, 2.9, 3.9, 3.6, 3.…\n$ DAILY_AQI_VALUE                  &lt;dbl&gt; 10, 15, 13, 23, 12, 16, 15, 16, 15, 1…\n$ CBSA_NAME                        &lt;chr&gt; \"Portland-Vancouver-Hillsboro, OR-WA\"…\n\n\nMonthly averages of PM2.5 daily mean concentrations in Portland-Vancouver-Hillsboro from 2017 to 2023:\n\n\nRows: 84\nColumns: 2\n$ month_year &lt;chr&gt; \"2017-01\", \"2017-02\", \"2017-03\", \"2017-04\", \"2017-05\", \"201…\n$ avg_pm2.5  &lt;dbl&gt; 9.776526, 5.328638, 3.842800, 3.857500, 3.912355, 3.677011,…\n\n\n\nWe wanted to explore environmental data because it can be considered a “subversive” topic by some audiences. We believe it is important to study environmental trends in order to confirm or deny common arguments and observe relationships between anthropogenic activity and environmental impacts. We chose this dataset because it is accessible through the EPA’s website and can be easily filtered from the AQI monitoring webpage by type of pollutant, geographic area, and year.\nOur two questions are:\n\n\nHow did the COVID pandemic lockdown impact air quality in the Portland Metropolitan area?\nIs there a correlation between wildfire season and air quality fluctuations in the Portland Metropolitan area?\n\n\nFor our first question, we will be visualizing changes over time for the 5 major pollutants above in the Portland-Vancouver-Hillsboro area. We will be using data from 2017 to 2023 in order to get a sense of what the air quality looked like before the pandemic, during the pandemic (highlighting 2020-2021 as prime lockdown years), and seeing if pollutant concentrations increased after lockdown restrictions were lifted (from 2022-2023). We will plot monthly averages for each pollutant and graph the changes across the 7 year time span on a line graph, with multiple lines for each pollutant. If there are too many observations we may split the data into three graphs (before, during, and after lockdown).\n\nFor our second question, we will plot observations for the first, 15th, and last day of each month across one year. We will observe whether there is a stark increase in pollutant concentrations during the wildfire season (July-September) and have multiple lines to see which pollutants in particular increase. We will make a graph for each year to check for precision."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Project Write-up",
    "section": "",
    "text": "Your write-up should consist of three parts:\n\n\nBrief introduction to the dataset. You may repeat some of the information about the dataset provided in the introduction to the dataset on the TidyTuesday repository, for instance. Imagine that your project is a standalone document and the grader has no prior knowledge of the dataset.\n\n\nThe title should relate to the question you’re answering.\n\n\nIntroduction to the question and what parts of the dataset are necessary to answer the question. Also discuss why you’re interested in this question.\n\n\n\nDescribe what types of plots you are going to make to address your question. For each plot, provide a clear explanation as to why this plot (e.g. boxplot, barplot, histogram, etc.) is best for providing the information you are asking about. The two plots should be of different types, and at least one of the two plots needs to use either color mapping or facets.\n\n\n\nIn this section, provide the code that generates your plots. Use scale functions to provide nice axis labels and guides. You are welcome to use theme functions to customize the appearance of your plot, but you are not required to do so. All plots must be made with ggplot2. Do not use base R or lattice plotting functions.\n\n\n\nIn the Discussion section, interpret the results of your analysis. Identify any trends revealed (or not revealed) by the plots. Speculate about why the data looks the way it does.\n\n\n\n\nSame structure outlined for Question 1, but for your new question. And the title should relate to the question you’re answering.\nWe encourage you to be concise. A paragraph should typically not be longer than 5 sentences.\nYou are not required to perform any statistical tests in this project, but you may do so if you find it helpful to answer your question."
  },
  {
    "objectID": "index.html#introduction-1-2-paragraphs",
    "href": "index.html#introduction-1-2-paragraphs",
    "title": "Project Write-up",
    "section": "",
    "text": "Brief introduction to the dataset. You may repeat some of the information about the dataset provided in the introduction to the dataset on the TidyTuesday repository, for instance. Imagine that your project is a standalone document and the grader has no prior knowledge of the dataset.\n\n\nThe title should relate to the question you’re answering.\n\n\nIntroduction to the question and what parts of the dataset are necessary to answer the question. Also discuss why you’re interested in this question.\n\n\n\nDescribe what types of plots you are going to make to address your question. For each plot, provide a clear explanation as to why this plot (e.g. boxplot, barplot, histogram, etc.) is best for providing the information you are asking about. The two plots should be of different types, and at least one of the two plots needs to use either color mapping or facets.\n\n\n\nIn this section, provide the code that generates your plots. Use scale functions to provide nice axis labels and guides. You are welcome to use theme functions to customize the appearance of your plot, but you are not required to do so. All plots must be made with ggplot2. Do not use base R or lattice plotting functions.\n\n\n\nIn the Discussion section, interpret the results of your analysis. Identify any trends revealed (or not revealed) by the plots. Speculate about why the data looks the way it does.\n\n\n\n\nSame structure outlined for Question 1, but for your new question. And the title should relate to the question you’re answering.\nWe encourage you to be concise. A paragraph should typically not be longer than 5 sentences.\nYou are not required to perform any statistical tests in this project, but you may do so if you find it helpful to answer your question."
  },
  {
    "objectID": "presentation.html#outline",
    "href": "presentation.html#outline",
    "title": "Project Presentation",
    "section": "Outline",
    "text": "Outline\n\nYour presentation should generally follow the same structure as your write-up.\nEach team will have 5 minutes for their presentation, and each team member must speak (roughly equally) during this time.\nYour presentation will be created using Quarto, which allows you to write slides using the same coding structure you’re used to with Rmarkdown.\nThis is a sample Quarto slide template you can edit to your desire to create your presentation."
  },
  {
    "objectID": "presentation.html#rubric",
    "href": "presentation.html#rubric",
    "title": "Project Presentation",
    "section": "Rubric",
    "text": "Rubric\n\nRoughly I recommend 1 slide for introduction, 2 slides for Question 1, ans 2 slides for Question 2.\n\nYou can imagine spending roughly one minute on each slide.\nYou should feel free to have more (or fewer) slides.\n\nYour evaluation will be based on your content, professionalism (including sticking to time), and your performance during the Q&A (question and answer). We don’t care how many slides you use to do this."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This project is developed by Adrien as a template for the first mini-project of Math/Stat 241 at Reed College."
  },
  {
    "objectID": "Question1.html",
    "href": "Question1.html",
    "title": "Activity 9: Dates and times",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "Question2.html",
    "href": "Question2.html",
    "title": "Portland Area Air Quality",
    "section": "",
    "text": "#data from csvs (will need to change location stuff)\nfires_full &lt;- read_csv(\"~/Documents/GitHub/project01-ojkp/air data/fires_full.csv\")\n\nRows: 2191 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (1): tot_ProAcres\ndate (1): ReportDate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npollutants_pdx &lt;- read_csv(\"~/Documents/GitHub/project01-ojkp/air data/pollutants_pdx.csv\", \n    col_types = cols(Source = col_skip(), \n        `Site ID` = col_skip(), POC = col_skip(), \n        `Site Name` = col_skip(), DAILY_OBS_COUNT = col_skip(), \n        PERCENT_COMPLETE = col_skip(), AQS_PARAMETER_CODE = col_skip(), \n        CBSA_CODE = col_skip(), CBSA_NAME = col_skip(), \n        STATE_CODE = col_skip(), STATE = col_skip(), \n        COUNTY_CODE = col_skip(), COUNTY = col_skip(), \n        SITE_LATITUDE = col_skip(), SITE_LONGITUDE = col_skip()))\n\n\n#pollutant data stuff/wrangling\npollutants_pdx &lt;- pollutants_pdx %&gt;%\n  rename(NO2_reading = \"Daily Max 1-hour NO2 Concentration\",\n         Ozone_reading = \"Daily Max 8-hour Ozone Concentration\",\n         CO_reading = \"Daily Max 8-hour CO Concentration\",\n         SO2_reading = \"Daily Max 1-hour SO2 Concentration\",\n         PM2.5_reading = \"Daily Mean PM2.5 Concentration\",\n         )\n\npollutants_pdx_melt &lt;- pollutants_pdx %&gt;%\n  pivot_longer(cols = c(NO2_reading, CO_reading, SO2_reading, PM2.5_reading, Ozone_reading),\n               names_to = \"variable\",\n               values_to = \"reading\") %&gt;%\n  filter(!is.na(reading))\n\npollutants_pdx_melt$Date &lt;- mdy(pollutants_pdx_melt$Date)\n\npollutants_pdx_melt &lt;- pollutants_pdx_melt %&gt;%\n  mutate(year = year(Date))\n\n\n#graphing fires\n#considering changing severity of fire seasons over years\nggplot(data = fires_full, aes(x=ReportDate, y = tot_ProAcres)) +\n  geom_line()\n\n\n\nfires_full &lt;- fires_full %&gt;%\n  mutate(year = year(ReportDate))\n\nggplot(data = fires_full, aes(x=ReportDate, y = tot_ProAcres)) +\n  geom_line() +\n  facet_wrap(~year, scales = \"free_x\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size=5))\n\n\n\n\n\n# In the proposal we said we'd plot observations for the first, 15th, and last day of each month across one year, and observe whether there is a stark increase in pollutant concentrations during the wildfire season (July-September) and have multiple lines to see which pollutants in particular increase. \n#instead of separate lines, separate graphs for unit/interpretability reasons\n\n#data for first 15th and last day of month\npollutants_FML &lt;- pollutants_pdx_melt %&gt;%\n  mutate(day = day(Date)) %&gt;%\n  filter(day == 1 | day == 15 | day == max(day)) %&gt;%\n  select(-day)  \n\n#2017, considering trends in just one year, can be altered for whichever year\nggplot(pollutants_FML %&gt;% \n         filter(year == 2017), aes(x = Date, y = reading, color = variable)) +\n  geom_line() +\n  facet_wrap(~ variable + year, scales = \"free\") +\n  labs(title = \"Observations for First, 15th, and Last Day of Each Month\",\n       x = \"Date\", y = \"Reading\") +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1, size=5))\n\n\n\n#on one super mega huge giant ggplot to see how this holds up across years\nggplot(pollutants_FML, aes(x = Date, y = reading, color = variable)) +\n  geom_line() +\n  facet_wrap(~ variable + year, scales = \"free\") +\n  labs(title = \"Observations for First, 15th, and Last Day of Each Month\",\n       x = \"Date\", y = \"Reading\") +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1, size=5))\n\n\n\n\n\n#okay... \n#this is a visualization with all the pollutants over all the years compared to wildfires. surely there's something to be gained from this. maybe not tho.\n\nplots &lt;- lapply(unique(pollutants_pdx_melt$variable), function(var) {\n  ggplot(data = pollutants_pdx_melt[pollutants_pdx_melt$variable == var, ], aes(x = Date, y = reading)) +\n    geom_line() +\n    labs(title = paste(\"Reading for\", var),\n         x = \"Date\", y = \"Reading\")\n})\n\nfire_plot &lt;- ggplot(data = fires_full, aes(x = ReportDate, y = tot_ProAcres)) +\n  geom_line(color = \"red\") +\n  labs(title = \"Total Protected Acres Burnt\",\n       x = \"Date\", y = \"Total Protected Acres\")\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ngrid.arrange(arrangeGrob(grobs = plots, ncol = 1), fire_plot, heights = c(4, 1))\n\n\n\n#this is that same nonsense but just for 2020:\npollutants_2020 &lt;- pollutants_pdx_melt %&gt;%\n  filter(year(Date) == 2020)\n\nplots &lt;- lapply(unique(pollutants_2020$variable), function(var) {\n  ggplot(data = pollutants_2020[pollutants_2020$variable == var, ], aes(x = Date, y = reading)) +\n    geom_line() +\n    labs(title = paste(\"Reading for\", var, \"in 2020\"),\n         x = \"Date\", y = \"Reading\")\n})\n\nfire_plot &lt;- ggplot(data = fires_full[fires_full$year == 2020, ], aes(x = ReportDate, y = tot_ProAcres)) +\n  geom_line(color = \"red\") +\n  labs(title = \"Total Protected Acres Burnt in 2020\",\n       x = \"Date\", y = \"Total Protected Acres\")\n\ngrid.arrange(arrangeGrob(grobs = plots, ncol = 1), fire_plot, heights = c(4, 1))\n\n\n\n\n\n#summary statistics\n#correlation in 2020 between protected area burnt and each pollutant readings\n\n#aggregating data + merging\npollutant_small20 &lt;- pollutants_2020 %&gt;%\n  group_by(Date, variable) %&gt;%\n  summarise(reading = mean(reading, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'Date'. You can override using the\n`.groups` argument.\n\nfires_small20 &lt;- fires_full[fires_full$year == 2020, ] %&gt;%\n  group_by(ReportDate) %&gt;%\n  summarise(tot_ProAcres = sum(tot_ProAcres, na.rm = TRUE)) %&gt;%\n  ungroup()\n\nmerged_data &lt;- merge(pollutant_small20, fires_small20, by.x = \"Date\", by.y = \"ReportDate\", all = TRUE)\n\n\n# Calculate correlation coefficients for each pollutant variable\ncorrelations &lt;- lapply(unique(merged_data$variable), function(var) {\n  pollutant_data &lt;- merged_data %&gt;%\n    filter(variable == var)\n  correlation &lt;- cor.test(pollutant_data$reading, pollutant_data$tot_ProAcres)$estimate\n  data.frame(Pollutant = var, Correlation = correlation)\n})\n\ncorrelations_df &lt;- do.call(rbind, correlations)\n\nprint(correlations_df)\n\n         Pollutant   Correlation\ncor     CO_reading  0.0004795176\ncor1   NO2_reading -0.0679820673\ncor2 Ozone_reading  0.1522080235\ncor3 PM2.5_reading  0.0060174558\ncor4   SO2_reading -0.0150541533"
  }
]